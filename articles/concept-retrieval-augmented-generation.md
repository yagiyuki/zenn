---
title: "【中学生でもわかるレベルで説明】LLMを強化する『RAG』の概念・仕組み・重要性を解説"
emoji: "💡"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AI", "LLM", "RAG", "機械学習", "ChatGPT"]
published: false # false: 下書き / true: 公開
---

## はじめに

ChatGPTをはじめとする生成AIは、大規模言語モデル（LLM）という技術がベースになっています。これは、**ものすごく大量の文章データを読み込んで、「次にどんな言葉が来るか？」を予測する訓練をした、とても賢いAI**のことです。人間が書いたような自然な文章を作り出したり、質問に答えたりできます。
しかし、LLMを実際のサービスなどで使おうとすると、ハルシネーション（もっともらしい嘘をつくこと）を起こしたり、特定の専門知識や最新情報を知らなかったりする課題に直面することが少なくありません。

これらの課題に対する有力な解決策として注目されているのが「**RAG（Retrieval-Augmented Generation）**」というアーキテクチャパターン（仕組みの設計図）です。

この記事では、「RAGという言葉は聞いたことがあるけれど、具体的にどのようなもので、なぜ重要なのかよく知らない」という方を対象に、RAGの基本的な概念、仕組み、そしてその重要性について解説します。
すでに似たような記事は世の中にたくさんありますが、この記事では特に基本的な概念にフォーカスし、中学生にも伝わるくらいの分かりやすさを目指しました。

## RAGとは？ - LLMを外部知識で強化するアーキテクチャ

RAGは「**Retrieval-Augmented Generation**」の略で、日本語では「**検索拡張生成**」と訳されます。その名の通り、以下の要素を組み合わせた考え方です。

1.  **Retrieval (検索):** 外部の知識ソースから関連情報を**検索**する。
2.  **Augmented (拡張):** 検索した情報でLLMへの入力を**拡張**する（情報を付け加えてパワーアップさせる）。
3.  **Generation (生成):** 拡張された情報をもとにLLMがテキストを**生成**する。

つまりRAGとは、**LLMがテキストを生成する際に、まず外部の知識ソースから関連情報を検索し、その検索結果（コンテキスト）をLLMへの入力（プロンプト）に含めることで、生成されるテキストの品質（事実性、関連性、鮮度など）を向上させる**ことを目的としています。

![RAG全体の仕組みを表す図](/images/rag_flow.png)

:::message alert
**重要なポイント**
RAGは特定のライブラリやプロダクト名ではなく、あくまで **「設計上の考え方」** です。この考え方に基づいて、様々な具体的な技術（データベース、検索アルゴリズム、LLMモデルなど）を組み合わせてシステムが構築されます。
:::

## なぜRAGは重要なのか？ - LLMの弱点を補う

RAGの役割は、外部から検索してきた**関連性の高い知識を動的に（＝その都度）注入**することで、LLM単体の限界を補い、より高品質な出力を実現することにあります。
では、なぜその役割が重要なのか？ それはLLMが持ついくつかの性質（弱点）が理由です。

1.  **知識の鮮度の担保:** 世の中の情報は常に新しくなっていきます。LLMは学習した時点までの知識しか持っていません。RAGを使えば、外部の最新情報データベースなどを検索することで、常に新しい情報に基づいた回答が可能になります。
2.  **ドメイン固有/非公開知識の活用:** 企業内の情報や、特定の専門分野の知識は、一般的なLLMには含まれていません。RAGなら、社内文書データベースなどを知識ソースとして検索することで、これらの情報に基づいた回答を生成できます。
3.  **ハルシネーションの抑制:** LLMは、知らないことや不確かなことについて質問されると、事実に基づかない「もっともらしい嘘（ハルシネーション）」を作り出してしまうことがあります。RAGでは、回答の根拠となる情報を外部から検索してLLMに与えるため、このようなハルシネーションを減らす効果が期待できます。

これらの理由から、LLMをより信頼性が高く、実用的な形でアプリケーションに組み込む上で、RAGは現在デファクトスタンダードとも言える重要な技術要素となっています。

:::message alert
**課題解決のアプローチとしてRAG以外は考えられないのか？**

結論から言うと、他のアプローチも**考えられます**。
主に以下の2つの方法があります。

1.  **LLM自体に追加学習させる（ファインチューニング、継続事前学習など）**
2.  **プロンプト（ユーザーがLLMに渡す指示文）に直接、必要な知識を大量に入れる**

これら2つのアプローチにも利点はありますが、以下のような欠点や難しさもあります。

* **追加学習の欠点:**
    * **高コスト:** 大量の計算資源（GPUなど）と時間が必要で、非常にコストがかかります。
    * **知識の静的さ:** 一度学習させても、その知識は次の学習まで固定されてしまい、リアルタイムな情報更新には向きません。
    * **破滅的忘却:** 新しい知識を学習させると、元々持っていた知識や能力を忘れてしまう可能性があります。
    * **データ準備:** 高品質な学習データを大量に用意する必要があります。

* **プロンプトに直接知識を入れる欠点:**
    * **コンテキストウィンドウの制限:** LLMが一度に処理できる情報量（コンテキストウィンドウ）には上限があり、大量の知識をすべて入れることはできません。
    * **関連情報の選択:** そもそも、特定の質問に対して「どの知識をプロンプトに入れるべきか」を判断する仕組みが必要です（結局、何らかの検索＝Retrievalが必要になりがちです）。
    * **効率とコスト:** 毎回大量の情報をプロンプトとして送信するのは、通信量やAPI利用料（トークン数課金）の面で非効率・高コストになります。

もちろん、これらのアプローチが有効な場面もあります。例えば、特定の文体や応答スタイルを学習させるならファインチューニングが適している場合があります。

:::

## RAGの歴史的背景 - 2020年の論文が転換点

おそらくですが、現在の「RAG」として広く知られるようになった直接的なきっかけは、Meta AI（当時はFacebook AI）で発表された論文「**Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**」と思います。
https://arxiv.org/abs/2005.11401?ref=blog-ja.allganize.ai

2020年といえばまだChatGPTが出る2年くらい前の話です。
RAGはChatGPTが出てから急に認知されるようになった技術で、いきなり出てきたみたいな印象があるかもしれませんが、その現代的な概念自体は、生成AIブームの少し前にすでに提案されていた、ということですね。

## まとめ

本記事では、LLMを強化するアーキテクチャパターンであるRAG（検索拡張生成）の基本について解説しました。

* RAGは「検索」＋「生成」を組み合わせる**設計概念**。
* 外部知識を検索し、LLMへの入力に加えることで、**LLMの弱点を補い、回答品質を向上**させる。
* **知識の鮮度維持、専門知識の活用、ハルシネーション抑制**に有効。
* 他のアプローチ（追加学習、プロンプトへの知識注入）と比較して、**外部知識参照においては実用的な利点が多い**。
* 2020年のMeta AIの論文が現代的なRAGの基礎となり、**生成AIの普及とともにその重要性が高まっている**。

本記事が、RAGへの理解を深める一助となれば幸いです。
